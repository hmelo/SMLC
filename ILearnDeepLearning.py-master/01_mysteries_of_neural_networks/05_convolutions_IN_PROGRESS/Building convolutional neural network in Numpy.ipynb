{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building convolutional neural network in Numpy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author: Piotr Skalski***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "# size of the photo\n",
    "PHOTO_SIZE = 28\n",
    "# number of pixels in the photo\n",
    "PIXEL_NUMBER = PHOTO_SIZE * PHOTO_SIZE\n",
    "# neural network architecture\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": PIXEL_NUMBER, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 1000, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 1000, \"output_dim\": 500, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 500, \"output_dim\": 500, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 500, \"output_dim\": 10, \"activation\": \"softmax\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist_dataset():\n",
    "    # The MNIST data set is available at http://yann.lecun.com, let's use curl to download it\n",
    "    if not os.path.exists(\"train-images-idx3-ubyte\"):\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "        !gunzip t*-ubyte.gz\n",
    "        \n",
    "    # Let's use loadlocal_mnist available in mlxtend.data to get data in numpy array form.\n",
    "    X1, y1 = loadlocal_mnist(\n",
    "        images_path=\"train-images-idx3-ubyte\", \n",
    "        labels_path=\"train-labels-idx1-ubyte\")\n",
    "\n",
    "    X2, y2 = loadlocal_mnist(\n",
    "        images_path=\"t10k-images-idx3-ubyte\", \n",
    "        labels_path=\"t10k-labels-idx1-ubyte\")\n",
    "    \n",
    "    # We normalize the brightness values for pixels\n",
    "    X1 = X1.reshape(X1.shape[0], -1) / 255\n",
    "    X2 = X2.reshape(X2.shape[0], -1) /255\n",
    "\n",
    "    # Combine downloaded data bundles\n",
    "    X = np.concatenate([X1, X2])\n",
    "    y = np.concatenate([y1, y2])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function for labels one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(split_percentage):\n",
    "    # Download data\n",
    "    X, y = download_mnist_dataset()\n",
    "    # One hot encode labels\n",
    "    y = one_hot_encoding(y)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_percentage, random_state=42)\n",
    "    return np.transpose(X_train), np.transpose(X_test), np.transpose(y_train), np.transpose(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary function to display the selected data set element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, idx):\n",
    "    img = X[:, idx].reshape(28,28)\n",
    "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
    "    plt.title('true label: %d' % np.where(y[:, idx] != 0)[0][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation of the existing implementation to support multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    # Column wise softmax\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    return e_Z / e_Z.sum(axis = 0)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2D = np.array([[1, 2, 3, 6],\n",
    "                     [2, 4, 5, 6],\n",
    "                     [3, 8, 7, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.00242826, 0.01587624, 0.33333333],\n",
       "       [0.24472847, 0.01794253, 0.11731043, 0.33333333],\n",
       "       [0.66524096, 0.97962921, 0.86681333, 0.33333333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(scores2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    elif activation is \"softmax\":\n",
    "        activation_func = softmax\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cost_value(Y_hat, Y, eps = 0.001):\n",
    "#     m = Y_hat.shape[1]\n",
    "#     cost = -1 / m * (np.dot(Y, np.log(Y_hat + eps).T) + np.dot(1 - Y, np.log(1 - Y_hat  + eps).T))\n",
    "#     return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_cross_entropy_loss(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    loss = - np.sum(Y * np.log(Y_hat)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_prob_into_class(probs):\n",
    "#     probs_ = np.copy(probs)\n",
    "#     probs_[probs_ > 0.5] = 1\n",
    "#     probs_[probs_ <= 0.5] = 0\n",
    "#     return probs_\n",
    "\n",
    "\n",
    "# def get_accuracy_value(Y_hat, Y):\n",
    "#     Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "#     return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_accuracy(Y_hat, Y):\n",
    "    n_values = Y_hat.shape[0]\n",
    "    values = Y_hat.argmax(axis=0)\n",
    "    Y_hat_one_hot = np.eye(n_values)[values].T\n",
    "    return (Y_hat_one_hot == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    elif activation is \"softmax\":\n",
    "        # TEST OF COURSERA SOLUTION\n",
    "        backward_activation_func = lambda dA_curr, Z_curr: dA_curr\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture, eps = 0.000000000001):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "#     dA_prev = - (np.divide(Y, Y_hat + eps) - np.divide(1 - Y, 1 - Y_hat + eps))\n",
    "    # TEST OF COURSERA SOLUTION\n",
    "    dA_prev = Y_hat - Y\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "#         print(Y_hat.shape)\n",
    "#         print(Y.shape)\n",
    "        \n",
    "        cost = multi_class_cross_entropy_loss(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "#         print(cost)\n",
    "        \n",
    "        accuracy = multi_class_accuracy(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "#         print(accuracy)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if(i % 5 == 0):\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE = 10000\n",
    "X_train = X_train[:,:SUBSET_SIZE]\n",
    "y_train = y_train[:,:SUBSET_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 18.05769 - accuracy: 0.09350\n",
      "Iteration: 00005 - cost: 6.55857 - accuracy: 0.34620\n",
      "Iteration: 00010 - cost: 1.29068 - accuracy: 0.67030\n",
      "Iteration: 00015 - cost: 1.01207 - accuracy: 0.73030\n",
      "Iteration: 00020 - cost: 0.85441 - accuracy: 0.76560\n",
      "Iteration: 00025 - cost: 0.75597 - accuracy: 0.78840\n",
      "Iteration: 00030 - cost: 0.68273 - accuracy: 0.80950\n",
      "Iteration: 00035 - cost: 0.62498 - accuracy: 0.82370\n",
      "Iteration: 00040 - cost: 0.57798 - accuracy: 0.83580\n",
      "Iteration: 00045 - cost: 0.53846 - accuracy: 0.84670\n",
      "Iteration: 00050 - cost: 0.50445 - accuracy: 0.85660\n",
      "Iteration: 00055 - cost: 0.47479 - accuracy: 0.86430\n",
      "Iteration: 00060 - cost: 0.44866 - accuracy: 0.87120\n",
      "Iteration: 00065 - cost: 0.42536 - accuracy: 0.87710\n",
      "Iteration: 00070 - cost: 0.40445 - accuracy: 0.88430\n",
      "Iteration: 00075 - cost: 0.38556 - accuracy: 0.88860\n",
      "Iteration: 00080 - cost: 0.36836 - accuracy: 0.89520\n",
      "Iteration: 00085 - cost: 0.35262 - accuracy: 0.89900\n",
      "Iteration: 00090 - cost: 0.33812 - accuracy: 0.90310\n",
      "Iteration: 00095 - cost: 0.32469 - accuracy: 0.90650\n"
     ]
    }
   ],
   "source": [
    "params_values, cost_history, accuracy_history = train(X_train, y_train, NN_ARCHITECTURE, 100, 0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602857142857143\n"
     ]
    }
   ],
   "source": [
    "y_test_hat, _ = full_forward_propagation(X_test, params_values, NN_ARCHITECTURE)\n",
    "accuracy = multi_class_accuracy(y_test_hat, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "http://wiki.fast.ai/index.php/Log_Loss\n",
    "http://cs231n.github.io/neural-networks-case-study/\n",
    "http://saitcelebi.com/tut/output/part2.html\n",
    "https://deepnotes.io/softmax-crossentropy\n",
    "https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
